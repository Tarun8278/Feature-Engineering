{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b4bb19-2797-4297-a366-ff4e6d66da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 1 > What is a parameter \n",
    "\n",
    "# In **machine learning**, a **parameter** refers to a configuration variable that is learned from the training data and is used by a model to make predictions or decisions. These parameters are internal to the model and are adjusted during training to minimize the error (or loss) in predictions.\n",
    "\n",
    "# Here are the key points about parameters in machine learning:\n",
    "\n",
    "# 1. **Model Parameters**: These are the values that the model learns during training. For example:\n",
    "#    - In a linear regression model, the parameters are the **weights** (coefficients) and the **bias**.\n",
    "#    - In a neural network, the parameters include the **weights** of the connections between neurons and the **biases** for each neuron.\n",
    "\n",
    "# 2. **Learned During Training**: Machine learning algorithms adjust the parameters to reduce the difference between the predicted output and the actual target (i.e., minimize the loss function). For instance, during training, a model might use gradient descent to find the best parameter values.\n",
    "\n",
    "# 3. **Fixed vs. Tuned Parameters**:\n",
    "#    - **Fixed Parameters**: These parameters are set before training and are not changed by the algorithm. For example, the number of layers in a neural network or the kernel type in a support vector machine (SVM).\n",
    "#    - **Tuned Parameters**: These parameters are learned by the model during the training process. In the case of linear regression, the weights and intercept are tuned to fit the data.\n",
    "\n",
    "# ### Example:\n",
    "\n",
    "# In a simple **linear regression model**, the model tries to find the best parameters (slope and intercept) that represent the relationship between the input (features) and the output (target):\n",
    "# \\[\n",
    "# y = mx + b\n",
    "# \\]\n",
    "# - \\(m\\) (slope) and \\(b\\) (intercept) are the parameters that the algorithm learns.\n",
    "\n",
    "# ### Difference from Hyperparameters:\n",
    "\n",
    "# - **Parameters** are internal to the model and learned during training.\n",
    "# - **Hyperparameters** are set before training and control the learning process, such as the learning rate, number of epochs, and batch size.\n",
    "\n",
    "# In summary, parameters in machine learning are the internal variables that are adjusted during training to allow the model to make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37615858-a3aa-496a-be87-ddad8c2578ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # question 2 >> What is correlation? >> What does negative correlation mean?\n",
    "\n",
    "# ### What is Correlation?\n",
    "\n",
    "# **Correlation** refers to a statistical relationship between two or more variables, indicating how changes in one variable are associated with changes in another. It shows whether and how strongly pairs of variables are related.\n",
    "\n",
    "# Key points:\n",
    "# - **Positive correlation**: When both variables increase or decrease together.\n",
    "# - **Negative correlation**: When one variable increases while the other decreases.\n",
    "# - **No correlation**: When there is no discernible relationship between the variables.\n",
    "\n",
    "# Correlation is often measured by the **correlation coefficient**, which ranges from **-1 to 1**:\n",
    "# - **1**: Perfect positive correlation — as one variable increases, the other also increases in a perfectly linear manner.\n",
    "# - **-1**: Perfect negative correlation — as one variable increases, the other decreases in a perfectly linear manner.\n",
    "# - **0**: No correlation — no predictable relationship between the variables.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### What Does Negative Correlation Mean?\n",
    "\n",
    "# **Negative correlation** means that when one variable increases, the other variable tends to decrease, and vice versa. There is an **inverse relationship** between the two variables.\n",
    "\n",
    "# Key points about negative correlation:\n",
    "# - **Inverse relationship**: One variable moves in the opposite direction to the other.\n",
    "# - A negative correlation coefficient ranges from **-1 to 0**.\n",
    "#   - **-1**: Perfect negative correlation (when one variable increases, the other decreases in a perfectly linear manner).\n",
    "#   - **0**: No correlation, meaning no predictable relationship.\n",
    "\n",
    "# ### Example of Negative Correlation:\n",
    "# 1. **Temperature and Heating Costs**: As **temperature** increases, the need for heating decreases, leading to a **negative correlation** between these two variables.\n",
    "# 2. **Exercise and Weight**: As a person **increases exercise**, their **weight** might decrease (assuming other factors remain constant), representing a **negative correlation**.\n",
    "\n",
    "# In a graph of negative correlation, the data points typically form a downward-sloping line from left to right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7932b22-3126-4323-8172-dc4421ccda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 3 >> Define Machine Learning. What are the main components in Machine Learning\n",
    "\n",
    "# ### **What is Machine Learning?**\n",
    "\n",
    "# **Machine Learning (ML)** is a subset of artificial intelligence (AI) that focuses on the development of algorithms and models that allow computers to learn from and make predictions or decisions based on data, without being explicitly programmed. In other words, machine learning enables systems to improve their performance on a task through experience (i.e., by processing more data over time).\n",
    "\n",
    "# ### **Main Components in Machine Learning**\n",
    "\n",
    "# The core components involved in a typical machine learning process are:\n",
    "\n",
    "# 1. **Data**:\n",
    "#    - **Training Data**: Data used to train the machine learning model. This data contains input-output pairs that help the model learn the patterns or relationships.\n",
    "#    - **Test Data**: A separate set of data used to evaluate the model's performance and generalizability after training.\n",
    "\n",
    "# 2. **Algorithms/Models**:\n",
    "#    - The algorithms are mathematical procedures that define how the model learns from data. Different types of algorithms are used for different tasks, such as:\n",
    "#      - **Supervised Learning**: The model learns from labeled data (data with known outcomes).\n",
    "#      - **Unsupervised Learning**: The model learns from unlabeled data (no predefined labels or outputs).\n",
    "#      - **Reinforcement Learning**: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "# 3. **Features**:\n",
    "#    - Features are the individual variables or attributes in the data that are used as inputs for the machine learning model. The quality and relevance of features directly influence the performance of the model.\n",
    "\n",
    "# 4. **Model Training**:\n",
    "#    - Training a model involves using algorithms to adjust the model's parameters based on the training data. The goal is to minimize the error or loss between the model's predictions and the actual outcomes.\n",
    "\n",
    "# 5. **Evaluation Metrics**:\n",
    "#    - After training, the model’s performance is assessed using various metrics such as accuracy, precision, recall, F1-score, mean squared error (MSE), etc., depending on the type of problem (classification, regression, etc.).\n",
    "\n",
    "# 6. **Optimization**:\n",
    "#    - Optimization techniques are used to adjust the parameters of the model to minimize errors. For example, gradient descent is a commonly used optimization algorithm in machine learning.\n",
    "\n",
    "# 7. **Inference**:\n",
    "#    - Once the model is trained and evaluated, it can be used to make predictions or decisions on new, unseen data (called inference or prediction).\n",
    "\n",
    "# ### **Types of Machine Learning**:\n",
    "# 1. **Supervised Learning**: \n",
    "#    - The model is trained on labeled data, where the correct answer (output) is already known.\n",
    "#    - Examples: Classification (spam detection) and Regression (predicting house prices).\n",
    "\n",
    "# 2. **Unsupervised Learning**:\n",
    "#    - The model is given data without labels and must find patterns or structure on its own.\n",
    "#    - Examples: Clustering (customer segmentation) and Association (market basket analysis).\n",
    "\n",
    "# 3. **Reinforcement Learning**:\n",
    "#    - The model learns by interacting with an environment and receiving feedback through rewards or penalties based on its actions.\n",
    "#    - Examples: Game-playing algorithms (e.g., AlphaGo), robotics, autonomous driving.\n",
    "\n",
    "# 4. **Semi-supervised and Self-supervised Learning**:\n",
    "#    - A combination of supervised and unsupervised learning where the model is trained with both labeled and unlabeled data.\n",
    "\n",
    "# ### Summary:\n",
    "# Machine learning is the field of study where machines learn from data to improve performance on a specific task. The key components of ML include data, algorithms, features, training, evaluation, optimization, and inference. By using these components, a machine learning model can make predictions or decisions without explicit programming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c37a848-b27b-49eb-bdfe-ae30d14b8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 4 >> How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "# The **loss value** plays a crucial role in determining how well a machine learning model performs. It quantifies the difference between the model's predictions and the actual target values (the ground truth). Essentially, the **loss function** calculates how \"wrong\" the model's predictions are.\n",
    "\n",
    "# ### **How the Loss Value Helps Determine Model Performance:**\n",
    "\n",
    "# 1. **Quantifies Model Error**:\n",
    "#    - The loss function gives a numerical representation of the error between predicted and actual values. A **lower loss** indicates that the model's predictions are closer to the actual values, meaning the model is performing well. Conversely, a **higher loss** indicates greater error, meaning the model is not performing well.\n",
    "\n",
    "# 2. **Guides Model Training**:\n",
    "#    - During training, the model uses the loss value to adjust its parameters (e.g., weights in a neural network) to reduce the error. This process typically involves optimization algorithms like **gradient descent**, which iteratively updates the model's parameters to minimize the loss. Therefore, a decreasing loss over time typically indicates that the model is improving.\n",
    "\n",
    "# 3. **Helps Compare Models**:\n",
    "#    - The loss value allows you to compare different models or configurations. A model with a lower loss value is generally considered better because it is making more accurate predictions. However, it's important to also consider other factors like **overfitting** (when the model performs well on training data but poorly on new data).\n",
    "\n",
    "# 4. **Indicates Overfitting or Underfitting**:\n",
    "#    - By examining the loss on both the **training** and **validation/test** datasets, you can detect **overfitting** or **underfitting**:\n",
    "#      - **Overfitting**: When the model performs very well on the training data but poorly on validation/test data, it means the model has \"memorized\" the training data and is not generalizing well. This may be indicated by a low training loss but high validation loss.\n",
    "#      - **Underfitting**: When the model performs poorly on both the training and validation/test data, it means the model is too simple or not learning enough from the data. This may be indicated by a high loss on both training and validation datasets.\n",
    "\n",
    "# 5. **Loss Function Types**:\n",
    "#    - The loss function used depends on the type of machine learning problem:\n",
    "#      - **Regression**: For continuous values (e.g., predicting house prices), common loss functions include **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**.\n",
    "#      - **Classification**: For discrete categories (e.g., classifying images), common loss functions include **Cross-Entropy Loss** or **Categorical Cross-Entropy**.\n",
    "#    - The choice of loss function affects how well the model performs and how its errors are penalized during training.\n",
    "\n",
    "# ### **Key Takeaways**:\n",
    "# - **Low loss**: The model is making accurate predictions and performing well.\n",
    "# - **High loss**: The model is performing poorly, with predictions far from the actual values.\n",
    "# - **Loss decrease during training**: Indicates the model is learning and improving.\n",
    "# - **Loss comparison across models**: Helps to choose the best-performing model.\n",
    "\n",
    "# In summary, the loss value is a direct measure of the accuracy of a model’s predictions. A good model is typically one with a low loss value, and monitoring how the loss changes over time during training helps ensure that the model is improving and generalizing well to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bff8a2f-c29a-4fa7-9f35-38e120e3c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 5 >> What are continuous and categorical variables?\n",
    "\n",
    "# In statistics and machine learning, **variables** are the characteristics or attributes that we measure or observe in a dataset. These variables can be broadly classified into two types: **continuous** and **categorical**.\n",
    "\n",
    "# ### 1. **Continuous Variables**:\n",
    "\n",
    "# - **Definition**: Continuous variables, also known as **quantitative variables**, are variables that can take an infinite number of values within a given range. These variables are typically numeric and can be measured on a scale.\n",
    "  \n",
    "# - **Key Characteristics**:\n",
    "#   - **Infinite possibilities**: Continuous variables can have an infinite number of values between any two points. For example, height can be 170.5 cm, 170.55 cm, 170.555 cm, and so on.\n",
    "#   - **Fractional values**: They can take fractional or decimal values.\n",
    "#   - **Measured**: Continuous variables are usually the result of a measurement (e.g., weight, temperature, or time).\n",
    "\n",
    "# - **Examples**:\n",
    "#   - **Height**: A person’s height could be 170 cm, 170.1 cm, 170.12 cm, etc.\n",
    "#   - **Weight**: A person's weight could be 60.5 kg, 60.55 kg, and so on.\n",
    "#   - **Temperature**: The temperature can be 23.4°C, 23.45°C, etc.\n",
    "#   - **Time**: The time it takes for a process to finish (e.g., 15.2 seconds, 15.25 seconds).\n",
    "\n",
    "# ### 2. **Categorical Variables**:\n",
    "\n",
    "# - **Definition**: Categorical variables, also known as **qualitative variables**, represent characteristics or labels that can be divided into categories. These variables can take on a limited, fixed number of values or categories.\n",
    "\n",
    "# - **Key Characteristics**:\n",
    "#   - **Limited number of categories**: The values that a categorical variable can take are usually predefined and distinct from one another.\n",
    "#   - **Non-numeric**: While categorical variables can sometimes be encoded numerically (e.g., 1 = \"Male\", 2 = \"Female\"), the numbers themselves do not carry any quantitative meaning.\n",
    "#   - **Descriptive**: Categorical variables are often descriptive in nature and represent labels or categories.\n",
    "\n",
    "# - **Examples**:\n",
    "#   - **Gender**: Categories could be \"Male\", \"Female\", or \"Non-binary\".\n",
    "#   - **Color**: Categories could be \"Red\", \"Blue\", \"Green\".\n",
    "#   - **Country**: Categories could be \"USA\", \"India\", \"Germany\".\n",
    "#   - **Marital Status**: Categories could be \"Single\", \"Married\", \"Divorced\".\n",
    "\n",
    "# ### **Types of Categorical Variables**:\n",
    "\n",
    "# Categorical variables can be further divided into two types:\n",
    "\n",
    "# 1. **Nominal**: These categories do not have any inherent order or ranking. The categories are just different, without any order or priority.\n",
    "#    - **Examples**: Gender, Eye color, Nationality.\n",
    "   \n",
    "# 2. **Ordinal**: These categories have a meaningful order or ranking, but the differences between the categories are not quantifiable.\n",
    "#    - **Examples**: Education level (\"High School\", \"Undergraduate\", \"Graduate\"), Rating scale (\"Poor\", \"Average\", \"Excellent\").\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary:\n",
    "\n",
    "# - **Continuous Variables**:\n",
    "#   - Numeric.\n",
    "#   - Can take an infinite number of values within a range.\n",
    "#   - Examples: Height, weight, temperature, time.\n",
    "\n",
    "# - **Categorical Variables**:\n",
    "#   - Represent categories or labels.\n",
    "#   - Can be **Nominal** (no order) or **Ordinal** (with order).\n",
    "#   - Examples: Gender, country, color, education level.\n",
    "\n",
    "# Understanding the difference between continuous and categorical variables is important for selecting the appropriate statistical methods or machine learning models for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35b7b515-6a24-472f-8aa6-582f1c28cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859ddc4-aaf2-4548-a739-0b9c7884e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# question 6 >> How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "# Handling categorical variables is an important step in **machine learning** because most machine learning algorithms require numerical input data. Categorical variables, such as labels or categories, need to be transformed into a numerical form to be used effectively by these algorithms.\n",
    "\n",
    "### Common Techniques to Handle Categorical Variables:\n",
    "\n",
    "# 1. **Label Encoding**:\n",
    "#    - **Definition**: Label encoding assigns a unique integer to each category in a categorical feature.\n",
    "#    - **How it works**: Each category is mapped to an integer value. For example:\n",
    "#      - Category \"Red\" = 0\n",
    "#      - Category \"Green\" = 1\n",
    "#      - Category \"Blue\" = 2\n",
    "#    - **Use case**: Label encoding is typically used for **ordinal** data (where the categories have an inherent order, like \"Low\", \"Medium\", \"High\").\n",
    "#    - **Limitations**: Label encoding can introduce a **false ordinal relationship** (e.g., the model might incorrectly interpret the order of categories as having numerical significance), which may not always be desirable for **nominal** categories (e.g., \"Red\", \"Green\", \"Blue\").\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(['Red', 'Green', 'Blue', 'Red'])\n",
    "\n",
    "\n",
    "# 2. **One-Hot Encoding**:\n",
    "#    - **Definition**: One-hot encoding creates a new binary (0 or 1) column for each category in the original categorical feature.\n",
    "#    - **How it works**: For each category in the feature, a new binary feature is created, where 1 indicates the presence of the category and 0 indicates its absence.\n",
    "#    - **Use case**: One-hot encoding is typically used for **nominal** categorical data (where categories have no meaningful order, like \"Red\", \"Green\", \"Blue\").\n",
    "#    - **Limitations**: One-hot encoding increases the dimensionality of the dataset, which can become computationally expensive when dealing with features with many categories.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({'Color': ['Red', 'Green', 'Blue', 'Red']})\n",
    "one_hot_encoded_data = pd.get_dummies(data, columns=['Color'])\n",
    "\n",
    "\n",
    "# 3. **Frequency Encoding**:\n",
    "#    - **Definition**: Frequency encoding assigns a value to each category based on how often that category occurs in the dataset.\n",
    "#    - **How it works**: Categories are replaced by the frequency or proportion of their occurrences in the dataset.\n",
    "#    - **Use case**: This technique is useful when there is a large number of categories, and one-hot encoding would cause a high-dimensional feature space. It can also be used for nominal data.\n",
    "#    - **Limitations**: Frequency encoding does not capture any inherent relationships between the categories and may not perform well when the frequency of categories is skewed.\n",
    "\n",
    "\n",
    "frequency_encoding = data['Color'].value_counts() / len(data)\n",
    "data['Color'] = data['Color'].map(frequency_encoding)\n",
    "\n",
    "\n",
    "# 4. **Target Encoding (Mean Encoding)**:\n",
    "#    - **Definition**: Target encoding replaces each category with the **mean** of the target variable for that category.\n",
    "#    - **How it works**: For each category, the model calculates the mean of the target variable (dependent variable) corresponding to that category.\n",
    "#    - **Use case**: Target encoding is often used for **categorical features** in **regression** or **classification** problems where the target variable is continuous or categorical.\n",
    "#    - **Limitations**: Target encoding can lead to **data leakage** if not handled carefully (when the encoding depends on the target variable), and it may not work well with high-cardinality features.\n",
    "\n",
    "\n",
    "target_mean = data.groupby('Color')['Target'].mean()\n",
    "data['Color'] = data['Color'].map(target_mean)\n",
    "\n",
    "\n",
    "# 5. **Binary Encoding**:\n",
    "#    - **Definition**: Binary encoding is a combination of label encoding and one-hot encoding. It first converts categories to numerical values, and then represents those numbers in binary format, splitting the binary digits into separate columns.\n",
    "#    - **How it works**: For example, if there are four categories, label encoding might map them to [0, 1, 2, 3]. Then, the binary representation of each value is split into columns: \n",
    "#      - `0 = [0, 0]`\n",
    "#      - `1 = [0, 1]`\n",
    "#      - `2 = [1, 0]`\n",
    "#      - `3 = [1, 1]`\n",
    "#    - **Use case**: Binary encoding is useful for categorical variables with a high number of unique categories, where one-hot encoding would create too many columns.\n",
    "#    - **Limitations**: Binary encoding can still result in a large number of features, and the binary representation might not always make sense for all machine learning models.\n",
    "\n",
    "\n",
    "import category_encoders as ce\n",
    "encoder = ce.BinaryEncoder(cols=['Color'])\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "\n",
    "\n",
    "# 6. **Hashing Encoding (Feature Hashing)**:\n",
    "#    - **Definition**: Hashing encoding uses a **hash function** to map each category to a fixed-size number of columns (hash buckets).\n",
    "#    - **How it works**: Categories are mapped to a fixed-size range using a hash function. This is especially useful for **high-cardinality** categorical variables (variables with a large number of categories).\n",
    "#    - **Use case**: Hashing is often used when there is a very large number of categories, and creating a one-hot encoding matrix would be computationally expensive.\n",
    "#    - **Limitations**: Hash collisions can occur (two different categories being assigned the same hash value), which might reduce the quality of the encoding.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "hasher = FeatureHasher(input_type='string', n_features=5)\n",
    "hashed_features = hasher.transform(data['Color'].astype(str))\n",
    "\n",
    "\n",
    "# ### Choosing the Right Technique:\n",
    "# The choice of encoding technique depends on:\n",
    "# - **Cardinality**: The number of categories in the variable.\n",
    "#   - High-cardinality categorical variables (many unique categories) may benefit from techniques like **binary encoding** or **hashing**.\n",
    "# - **Type of data**: Whether the categorical variable is **ordinal** (has an inherent order) or **nominal** (no order).\n",
    "#   - **Ordinal** variables can often use **label encoding** because the order matters.\n",
    "#   - **Nominal** variables typically use **one-hot encoding**, as there’s no inherent order.\n",
    "# - **Model type**: Some machine learning models (e.g., tree-based models like decision trees or random forests) can handle categorical variables directly, while others (e.g., linear models) require numerical inputs.\n",
    "\n",
    "# ### Summary:\n",
    "# Handling categorical variables in machine learning is essential to ensure that the model can process the data. Common techniques include **Label Encoding**, **One-Hot Encoding**, **Frequency Encoding**, **Target Encoding**, **Binary Encoding**, and **Hashing Encoding**. The choice of technique depends on the nature of the categorical data (ordinal or nominal), the number of categories, and the machine learning model being used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085dff9f-fb8f-4a96-90a1-1b5a0129cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 7 >> What do you mean by training and testing a dataset?\n",
    "\n",
    "# In machine learning, **training** and **testing** a dataset are crucial steps in the model development process. These steps are used to build and evaluate the model's performance.\n",
    "\n",
    "# ### 1. **Training a Dataset**:\n",
    "\n",
    "# **Training** refers to the process of teaching the machine learning model to learn patterns or relationships from a given set of data (known as the **training data**). During training, the model adjusts its parameters (like weights in a neural network or coefficients in linear regression) based on the input data and the expected output (the labels or target values).\n",
    "\n",
    "# #### Key Concepts:\n",
    "# - **Training Data**: The dataset used to train the model. It consists of both input features (independent variables) and corresponding target values (dependent variables).\n",
    "# - **Model Learning**: During training, the model uses the data to understand the relationships between the input features and the target values. The model’s parameters are adjusted to minimize the error or loss function (the difference between the predicted and actual outputs).\n",
    "# - **Overfitting Risk**: If the model becomes too complex or is trained too long, it might memorize the training data rather than learning general patterns, which can lead to overfitting. Overfitting means the model performs well on the training data but poorly on unseen data.\n",
    "\n",
    "# #### Training Process:\n",
    "# 1. The model is initialized (with random weights or parameters).\n",
    "# 2. The model makes predictions on the training data.\n",
    "# 3. The predictions are compared to the actual values (labels).\n",
    "# 4. The error is calculated (using a loss function).\n",
    "# 5. The model's parameters are updated to minimize the error (using optimization algorithms like gradient descent).\n",
    "# 6. The process repeats until the model converges or reaches a satisfactory level of performance.\n",
    "\n",
    "# ### 2. **Testing a Dataset**:\n",
    "\n",
    "# **Testing** refers to evaluating the performance of the trained model on new, unseen data, called the **testing data** (or validation data in some cases). The testing data is not used during the training phase. Its purpose is to provide an unbiased evaluation of the model’s ability to generalize to new data.\n",
    "\n",
    "# #### Key Concepts:\n",
    "# - **Testing Data**: This data is separate from the training data. It is used to test how well the model generalizes to data it hasn’t seen before.\n",
    "# - **Model Evaluation**: After training, the model is tested on the testing dataset to see how accurately it predicts the target values. The testing dataset helps assess the model’s **generalization** ability.\n",
    "# - **Metrics**: Common evaluation metrics include accuracy, precision, recall, F1 score, mean squared error (MSE), etc., depending on the type of machine learning problem (classification, regression).\n",
    "\n",
    "# #### Testing Process:\n",
    "# 1. The trained model is applied to the testing data.\n",
    "# 2. The model makes predictions based on the input features of the testing data.\n",
    "# 3. The predicted values are compared with the actual target values in the test set.\n",
    "# 4. Evaluation metrics are calculated to assess the model's performance.\n",
    "\n",
    "# ### **Why is Splitting Data into Training and Testing Important?**\n",
    "# - **Generalization**: The main goal is to ensure that the model can generalize well to new, unseen data. Testing on the training data alone would give an overly optimistic assessment of the model’s performance.\n",
    "# - **Avoid Overfitting**: If the model performs well only on the training data and not on the testing data, it indicates **overfitting**. This means the model has memorized the training data but cannot handle new data effectively.\n",
    "# - **Performance Assessment**: By testing the model on unseen data, we get an accurate measure of its ability to predict real-world data.\n",
    "\n",
    "# ### **Common Data Split**:\n",
    "# - The dataset is often split into two or three subsets:\n",
    "#   - **Training Set**: Usually 70%–80% of the total data is used for training the model.\n",
    "#   - **Testing Set**: Typically, 20%–30% of the data is set aside for testing the model’s performance.\n",
    "#   - Sometimes, a **Validation Set** is also used to tune hyperparameters during training, making the split:\n",
    "#     - **Training Set**: 60%–70%\n",
    "#     - **Validation Set**: 15%–20%\n",
    "#     - **Testing Set**: 15%–20%\n",
    "\n",
    "# ### **Summary**:\n",
    "# - **Training a Dataset**: The process where a model learns patterns from the training data, adjusting its parameters to minimize prediction errors.\n",
    "# - **Testing a Dataset**: The process of evaluating the trained model on new, unseen data (testing data) to check how well the model generalizes to real-world data.\n",
    "\n",
    "# Together, training and testing ensure that the model is capable of not just learning from historical data but also performing well on future data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2924dcc1-7dc9-45ae-854c-2215349115a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 8 >> What is sklearn.preprocessing? \n",
    "\n",
    "# `**sklearn.preprocessing**` is a module in **scikit-learn** (a popular machine learning library in Python) that provides various utilities and functions for **data preprocessing**. These preprocessing techniques are essential to prepare your data before feeding it into machine learning models.\n",
    "\n",
    "# The goal of preprocessing is to make the data suitable for model training, which can involve scaling, encoding, imputing missing values, and transforming features. Preprocessing helps improve model performance and ensures the model works efficiently.\n",
    "\n",
    "# ### **Common Functions and Classes in `sklearn.preprocessing`:**\n",
    "\n",
    "# 1. **Scaling and Normalization:**\n",
    "#    - **StandardScaler**: Standardizes the features by removing the mean and scaling to unit variance. This is useful when features have different scales, especially for algorithms like **Logistic Regression**, **SVMs**, and **KNN**.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import StandardScaler\n",
    "#      scaler = StandardScaler()\n",
    "#      X_scaled = scaler.fit_transform(X)\n",
    "#      ```\n",
    "#    - **MinMaxScaler**: Scales the features to a specified range (usually between 0 and 1). This is useful for algorithms that require a bounded range of values, such as neural networks.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import MinMaxScaler\n",
    "#      scaler = MinMaxScaler()\n",
    "#      X_scaled = scaler.fit_transform(X)\n",
    "#      ```\n",
    "#    - **RobustScaler**: Scales the features using the median and the interquartile range (IQR), which makes it robust to outliers.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import RobustScaler\n",
    "#      scaler = RobustScaler()\n",
    "#      X_scaled = scaler.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 2. **Encoding Categorical Variables:**\n",
    "#    - **LabelEncoder**: Encodes categorical labels into integers (numeric format). This is typically used for **target labels** (output variable).\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import LabelEncoder\n",
    "#      encoder = LabelEncoder()\n",
    "#      y_encoded = encoder.fit_transform(y)\n",
    "#      ```\n",
    "#    - **OneHotEncoder**: Converts categorical variables into one-hot encoded vectors. It creates binary columns for each category, useful for nominal categorical features (e.g., \"Red\", \"Green\", \"Blue\").\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import OneHotEncoder\n",
    "#      encoder = OneHotEncoder()\n",
    "#      X_encoded = encoder.fit_transform(X)\n",
    "#      ```\n",
    "#    - **OrdinalEncoder**: Encodes ordinal categorical variables, where there is a meaningful order or ranking (e.g., \"Low\", \"Medium\", \"High\").\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import OrdinalEncoder\n",
    "#      encoder = OrdinalEncoder()\n",
    "#      X_encoded = encoder.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 3. **Imputing Missing Data:**\n",
    "#    - **SimpleImputer**: This is used to handle missing values (NaNs) in the dataset by replacing them with a specified value such as the mean, median, or most frequent value.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import SimpleImputer\n",
    "#      imputer = SimpleImputer(strategy='mean')  # Options: 'mean', 'median', 'most_frequent', etc.\n",
    "#      X_imputed = imputer.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 4. **Binarization:**\n",
    "#    - **Binarizer**: Transforms features by thresholding them. It replaces values below a specified threshold with 0 and values above the threshold with 1. It is typically used to convert continuous variables into binary variables.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import Binarizer\n",
    "#      binarizer = Binarizer(threshold=0.0)\n",
    "#      X_binarized = binarizer.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 5. **Polynomial Features:**\n",
    "#    - **PolynomialFeatures**: Generates polynomial and interaction features. This can be useful when trying to capture non-linear relationships between features (e.g., for polynomial regression).\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import PolynomialFeatures\n",
    "#      poly = PolynomialFeatures(degree=2)  # Degree determines the highest degree of the polynomial\n",
    "#      X_poly = poly.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 6. **Power Transformations:**\n",
    "#    - **PowerTransformer**: Applies a power transformation to make the data more Gaussian-like (e.g., through log transformation or Yeo-Johnson transformation), which can improve the performance of certain machine learning algorithms that assume data is normally distributed.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import PowerTransformer\n",
    "#      transformer = PowerTransformer()\n",
    "#      X_transformed = transformer.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# 7. **Quantile Transformation:**\n",
    "#    - **QuantileTransformer**: Transforms features using quantile information to map the data to a uniform or normal distribution. This can be useful for handling skewed data.\n",
    "#      ```python\n",
    "#      from sklearn.preprocessing import QuantileTransformer\n",
    "#      transformer = QuantileTransformer(output_distribution='normal')\n",
    "#      X_transformed = transformer.fit_transform(X)\n",
    "#      ```\n",
    "\n",
    "# ### **When and Why to Use `sklearn.preprocessing`:**\n",
    "# - **Scaling**: When features have different ranges or units (e.g., income vs. age), models may not perform well unless the data is normalized or standardized.\n",
    "# - **Encoding**: Many machine learning models (like linear regression, decision trees, etc.) require numerical input, so categorical data must be converted into a numerical form.\n",
    "# - **Imputation**: Missing data is common in real-world datasets, and imputation techniques help fill in missing values, preventing errors in model training.\n",
    "# - **Non-linear Relationships**: Polynomial and power transformations allow models to capture more complex, non-linear relationships.\n",
    "# - **Outlier Handling**: Robust scalers are helpful when data contains outliers that would disproportionately affect models like linear regression or SVM.\n",
    "\n",
    "# ### **Summary**:\n",
    "# The **`sklearn.preprocessing`** module provides various tools for transforming data before feeding it into machine learning models. These tools help normalize, scale, encode, and handle missing values in the dataset, improving model performance and accuracy. Some common preprocessing tasks include **scaling** features, **encoding** categorical data, **imputing missing values**, and applying **transformations** like polynomial features or power transformations. Proper preprocessing is crucial for the success of machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae211e3-89f4-4477-a3a9-1efbec9b7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 09 >>  What is a Test set\n",
    "\n",
    "# In machine learning, a **test set** is a portion of the data that is used to evaluate the performance of a trained model. It is separate from the training set and the validation set.\n",
    "\n",
    "# - **Training Set**: The data used to train the model.\n",
    "# - **Validation Set**: The data used to tune the model's hyperparameters during training.\n",
    "# - **Test Set**: The data used to assess the model's performance after training and hyperparameter tuning.\n",
    "\n",
    "# The test set serves as an unbiased evaluation of the final model’s accuracy and generalization ability, helping to ensure that the model performs well on unseen data. Typically, the test set is kept completely separate from the training and validation sets to prevent data leakage and overfitting. \n",
    "\n",
    "# In short, after the model is trained and tuned, the test set provides an estimate of how the model will perform in real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791f5b5-7e96-4744-aba4-10f168211875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 10 >> How do we split data for model fitting (training and testing) in Python?\n",
    "#             >> How do you approach a Machine Learning problem?\n",
    "\n",
    "### Splitting Data for Model Fitting (Training and Testing) in Python:\n",
    "\n",
    "# In Python, the most common library used to split data into training and testing sets is **scikit-learn**. Specifically, you can use the `train_test_split()` function from the `sklearn.model_selection` module to perform this task. Here's how you can do it:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset: X = features, y = target/labels\n",
    "# X = # Your features\n",
    "# y = # Your target variable\n",
    "\n",
    "# Split data into training and testing sets (e.g., 80% for training and 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# - **X** represents the features (input data).\n",
    "# - **y** represents the target labels (output data).\n",
    "# - `test_size=0.2` means 20% of the data will be used as the test set, and the remaining 80% will be used as the training set.\n",
    "# - `random_state=42` ensures reproducibility of the split.\n",
    "\n",
    "# ### Approaching a Machine Learning Problem:\n",
    "\n",
    "# 1. **Define the Problem**:\n",
    "#    - Understand the problem and determine whether it is a **supervised learning** problem (classification or regression) or **unsupervised learning** (clustering, dimensionality reduction, etc.).\n",
    "   \n",
    "# 2. **Collect and Understand Data**:\n",
    "#    - **Data Collection**: Gather relevant data that can help solve the problem.\n",
    "#    - **Data Exploration**: Perform exploratory data analysis (EDA) to understand the dataset (summary statistics, visualizations).\n",
    "#    - **Data Cleaning**: Handle missing values, remove outliers, and deal with incorrect data.\n",
    "   \n",
    "# 3. **Preprocess the Data**:\n",
    "#    - **Feature Engineering**: Create new features or transform existing ones based on domain knowledge.\n",
    "#    - **Normalization/Scaling**: Normalize or scale features (for algorithms like SVM, k-NN, and neural networks that are sensitive to the scale of data).\n",
    "#    - **Encoding Categorical Variables**: Convert categorical variables to numerical format (e.g., one-hot encoding).\n",
    "#    - **Split the Data**: Use `train_test_split()` to divide the data into training and testing sets.\n",
    "\n",
    "# 4. **Select a Model**:\n",
    "#    - Choose an appropriate machine learning algorithm based on the problem type and dataset. Common algorithms include:\n",
    "#      - **Linear models** (e.g., Linear Regression, Logistic Regression)\n",
    "#      - **Decision trees** (e.g., Random Forest, XGBoost)\n",
    "#      - **Support Vector Machines (SVM)**\n",
    "#      - **Neural Networks**\n",
    "#      - **Clustering algorithms** (e.g., K-means for unsupervised tasks)\n",
    "   \n",
    "# 5. **Train the Model**:\n",
    "#    - Fit the chosen model to the training data (X_train, y_train).\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Example: Training a Random Forest Classifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 6. **Evaluate the Model**:\n",
    "#    - Use the test set (X_test, y_test) to evaluate the model’s performance using appropriate metrics.\n",
    "#      - **Classification metrics**: Accuracy, Precision, Recall, F1-score, ROC-AUC, etc.\n",
    "#      - **Regression metrics**: Mean Squared Error (MSE), R², Mean Absolute Error (MAE), etc.\n",
    "   \n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# 7. **Tune the Model**:\n",
    "#    - **Hyperparameter tuning**: Fine-tune the model using techniques like Grid Search or Randomized Search to optimize performance.\n",
    "   \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Grid Search for hyperparameter tuning\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "# 8. **Deploy and Monitor**:\n",
    "#    - Once the model performs well on test data, you can deploy it for real-world use.\n",
    "#    - Monitor the model over time to ensure it continues to perform as expected, especially if the data distribution changes.\n",
    "\n",
    "# ### Additional Steps:\n",
    "# - **Cross-validation**: You may use k-fold cross-validation during model training to ensure better model validation and reduce overfitting.\n",
    "# - **Feature selection**: Try to reduce dimensionality by removing irrelevant features to improve model performance.\n",
    "  \n",
    "# This process provides a comprehensive approach to building a machine learning model from problem definition to deployment and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd75d4a1-4b8b-4997-a350-218589c0ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 11 >> Why do we have to perform EDA before fitting a model to the data\n",
    "\n",
    "# Performing **Exploratory Data Analysis (EDA)** before fitting a machine learning model is a crucial step for several reasons. EDA helps to gain insights into the data and prepares it for the modeling process, ensuring better model performance and reducing potential issues later on. Here's why EDA is essential:\n",
    "\n",
    "# ### 1. **Understanding the Data**\n",
    "#    - **Identifying data types**: EDA helps you understand the structure of your data (numerical, categorical, datetime, etc.). This is important because different types of data require different preprocessing and modeling techniques (e.g., encoding for categorical variables, scaling for numerical features).\n",
    "#    - **Assessing data distribution**: By visualizing distributions (e.g., histograms, box plots), you can identify skewness, outliers, and patterns, which can guide data transformations (like normalization or log transformations) and model selection.\n",
    "\n",
    "# ### 2. **Identifying Missing or Inconsistent Data**\n",
    "#    - **Missing values**: EDA helps you identify any missing values in your dataset. Missing data can bias the model if not handled properly, and you need to decide whether to impute, drop, or handle them in another way.\n",
    "#    - **Inconsistent data**: You can detect inconsistencies, such as negative values for attributes that should only be positive (e.g., age, price), or incorrect categorical entries (e.g., misspelled category names). Addressing these issues is essential for building a robust model.\n",
    "\n",
    "# ### 3. **Understanding Relationships Between Variables**\n",
    "#    - **Correlation between features**: You can uncover how features are correlated (using correlation matrices, scatter plots, etc.). Strong correlations might indicate multicollinearity, which can affect model performance, especially for algorithms like linear regression. It may also suggest that some features can be dropped.\n",
    "#    - **Target relationships**: EDA allows you to explore the relationship between features and the target variable (e.g., using pair plots, bar plots, etc.). Understanding these relationships helps with feature selection and can suggest which features are likely to have predictive power.\n",
    "\n",
    "# ### 4. **Detecting Outliers and Anomalies**\n",
    "#    - Outliers can drastically affect the performance of certain models (e.g., linear regression, k-NN) by distorting the relationships between features. EDA allows you to spot outliers using box plots, histograms, or scatter plots and decide how to handle them (e.g., remove, cap, or transform).\n",
    "\n",
    "# ### 5. **Feature Engineering**\n",
    "#    - **Creating new features**: Based on the insights gained from EDA, you can engineer new features that might enhance the predictive power of the model. For example, you might create interaction terms, aggregate features, or extract date-related features.\n",
    "#    - **Identifying irrelevant features**: EDA also helps identify features that don’t contribute meaningfully to the prediction (e.g., constant or redundant features). This can lead to better model performance and faster training.\n",
    "\n",
    "# ### 6. **Improving Model Choice**\n",
    "#    - **Selecting the right model**: EDA can give you insights into the nature of the problem (e.g., regression, classification, clustering) and help you select appropriate algorithms. For example, if the data is highly imbalanced, you might consider techniques like SMOTE or models like Random Forest or XGBoost, which handle imbalances better.\n",
    "#    - **Scaling and transformations**: If your features have different scales or distributions, EDA will show you this, allowing you to apply techniques like normalization, standardization, or log transformations to improve model performance.\n",
    "\n",
    "# ### 7. **Checking Assumptions for Specific Models**\n",
    "#    - Certain machine learning models (e.g., linear regression, logistic regression) have underlying assumptions, such as linearity, homoscedasticity, and normality of residuals. EDA can help check these assumptions by visualizing residuals or the relationship between features and the target variable.\n",
    "  \n",
    "# ### 8. **Building Trust and Interpretability**\n",
    "#    - EDA helps you understand the data thoroughly, which is critical when interpreting and explaining the model's results. If you understand the underlying patterns and anomalies in the data, you will be in a better position to interpret the model's predictions and communicate findings to stakeholders.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### In summary, EDA is essential because:\n",
    "# - It helps identify and clean problematic data.\n",
    "# - It aids in selecting the right model and features.\n",
    "# - It reveals underlying patterns, relationships, and potential issues in the data.\n",
    "# - It guides data transformations and feature engineering to improve model performance.\n",
    "\n",
    "# Without EDA, you risk building a model on flawed or poorly understood data, which can lead to inaccurate predictions, overfitting, or poor generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8032a388-bb1f-4a23-821e-95ea361e8db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Hours_Studied  Scores\n",
      "Hours_Studied            1.0     1.0\n",
      "Scores                   1.0     1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# question 12 >> What is correlation\n",
    "\n",
    "# **Correlation** is a statistical measure that describes the strength and direction of a relationship between two or more variables. It tells you how closely the variables move together — whether they increase or decrease in a consistent manner.\n",
    "\n",
    "# In the context of machine learning and data analysis, **correlation** helps you understand how one feature (or variable) is related to another, and whether or not they can be used together in a model.\n",
    "\n",
    "# ### Types of Correlation:\n",
    "\n",
    "# 1. **Positive Correlation**:\n",
    "#    - When two variables increase or decrease together. In other words, as one variable increases, the other variable also increases, and vice versa.\n",
    "#    - **Example**: As the number of hours studied increases, the exam scores tend to increase. \n",
    "#    - **Correlation coefficient**: Between 0 and 1.\n",
    "\n",
    "# 2. **Negative Correlation**:\n",
    "#    - When one variable increases while the other decreases (or vice versa).\n",
    "#    - **Example**: As the temperature decreases, the number of layers of clothing worn increases.\n",
    "#    - **Correlation coefficient**: Between -1 and 0.\n",
    "\n",
    "# 3. **No Correlation**:\n",
    "#    - When there is no discernible pattern or relationship between the two variables.\n",
    "#    - **Example**: The number of hours someone watches TV might not affect their height.\n",
    "#    - **Correlation coefficient**: Close to 0.\n",
    "\n",
    "# ### **Correlation Coefficient**:\n",
    "# The correlation between two variables is usually quantified using a **correlation coefficient**, most commonly the **Pearson correlation coefficient**. It measures both the direction and strength of the linear relationship between two continuous variables.\n",
    "\n",
    "# - **Formula for Pearson correlation coefficient (r)**:\n",
    "#   \\[\n",
    "#   r = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(X_i\\) and \\(Y_i\\) are individual sample points for variables X and Y.\n",
    "#   - \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the mean values of X and Y.\n",
    "\n",
    "# ### **Interpreting the Correlation Coefficient (r):**\n",
    "# - **r = 1**: Perfect positive correlation (both variables increase together).\n",
    "# - **r = -1**: Perfect negative correlation (as one variable increases, the other decreases).\n",
    "# - **r = 0**: No linear correlation (no relationship).\n",
    "# - **0 < r < 1**: Positive correlation (the variables move in the same direction, but not perfectly).\n",
    "# - **-1 < r < 0**: Negative correlation (the variables move in opposite directions, but not perfectly).\n",
    "\n",
    "# ### **Visualization of Correlation**:\n",
    "# You can visualize the correlation between variables using:\n",
    "# - **Scatter plots**: To see the relationship between two variables.\n",
    "# - **Heatmaps**: To visualize the correlation matrix of multiple variables (often used with datasets containing many features).\n",
    "\n",
    "# ### Example in Python (using **pandas** and **seaborn**):\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
    "    'Scores': [55, 60, 65, 70, 75]\n",
    "})\n",
    "\n",
    "# Calculate correlation coefficient\n",
    "correlation = data.corr()\n",
    "print(correlation)\n",
    "\n",
    "# Plotting the correlation matrix using a heatmap\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### Why Correlation Matters in Machine Learning:\n",
    "# - **Feature Selection**: If two features are highly correlated (either positively or negatively), one may be redundant. This can lead to multicollinearity in models like linear regression, so removing one of the correlated features might improve the model's performance and stability.\n",
    "# - **Understanding Relationships**: Correlation helps you understand how different variables in your dataset relate to one another, which can guide your choice of features for modeling.\n",
    "\n",
    "# ### Important Note:\n",
    "# - **Correlation does not imply causation**: Just because two variables are correlated, it does not mean one causes the other. For example, there may be a strong correlation between ice cream sales and drowning, but they are not causally related. The relationship may be due to a third variable (like temperature).\n",
    "\n",
    "# In summary, correlation is a fundamental concept in data analysis that helps you understand the relationships between variables, guiding feature selection and informing model-building decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2d736c-0734-4952-915b-1dbf319a28a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # question 13 >> What does negative correlation mean\n",
    "\n",
    "# A **negative correlation** between two variables means that as one variable increases, the other tends to decrease, and vice versa. In other words, the two variables move in opposite directions.\n",
    "\n",
    "# ### Key Points:\n",
    "# - **Inverse Relationship**: In a negative correlation, if one variable increases, the other decreases. If one decreases, the other increases.\n",
    "# - **Correlation Coefficient**: The Pearson correlation coefficient for negative correlation is between **-1 and 0**.\n",
    "#   - A value of **-1** indicates a **perfect negative correlation**, meaning the two variables are perfectly inversely related (if one increases by a certain amount, the other decreases by the same amount).\n",
    "#   - A value closer to **0** means a weak or no linear negative relationship exists.\n",
    "  \n",
    "# ### Examples of Negative Correlation:\n",
    "\n",
    "# 1. **Temperature and Heater Usage**: As the outside temperature increases, the need for heating inside a house typically decreases.\n",
    "#    - When the temperature goes up (increase), heater usage tends to go down (decrease).\n",
    "#    - The correlation is negative.\n",
    "\n",
    "# 2. **Speed and Travel Time**: As the speed of a car increases, the time taken to reach a destination typically decreases (assuming constant distance).\n",
    "#    - Higher speed means less time.\n",
    "#    - The correlation is negative.\n",
    "\n",
    "# 3. **Price and Demand**: In many cases, as the price of a product increases, the demand for it tends to decrease (for most goods, following the law of demand).\n",
    "#    - Higher prices usually lead to lower demand.\n",
    "#    - The correlation is negative.\n",
    "\n",
    "# ### Visualizing Negative Correlation:\n",
    "# If you plot data with a negative correlation on a scatter plot, you would see that as one variable increases (along the x-axis), the other decreases (along the y-axis), typically forming a downward-sloping pattern.\n",
    "\n",
    "# #### Example of Negative Correlation on a Scatter Plot:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create data with negative correlation\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([10, 8, 6, 4, 2])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.title('Negative Correlation Example')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In this plot, as `x` increases, `y` decreases, showing a clear negative correlation.\n",
    "\n",
    "# ### Interpreting Negative Correlation in Data Science and Machine Learning:\n",
    "# - **Feature Selection**: If two features are negatively correlated, one might be redundant or not add new information. In some cases, you might choose to drop one feature to improve model performance and reduce complexity (e.g., in linear regression to avoid multicollinearity).\n",
    "# - **Understanding Relationships**: A negative correlation helps you understand how variables interact, which can inform feature engineering and model decisions.\n",
    "\n",
    "# ### Summary:\n",
    "# A negative correlation signifies an inverse relationship between two variables — as one increases, the other decreases. It's important for identifying and interpreting how variables relate to each other, and it plays a crucial role in data analysis and machine learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f7c979-3da0-44ad-a3f0-c2ba76ae0004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Hours_Studied  Scores  Age\n",
      "Hours_Studied            1.0     1.0  1.0\n",
      "Scores                   1.0     1.0  1.0\n",
      "Age                      1.0     1.0  1.0\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# question 14 >> How can you find correlation between variables in Python\n",
    "\n",
    "# In Python, you can easily find the correlation between variables using libraries like **pandas** and **NumPy**. The most commonly used method for calculating correlation is the **Pearson correlation coefficient**, which measures the linear relationship between two variables.\n",
    "\n",
    "# Here's how you can calculate and visualize the correlation between variables using Python:\n",
    "\n",
    "# ### 1. **Using Pandas**:\n",
    "# The `pandas` library has a built-in `.corr()` method that calculates the correlation between numerical columns of a DataFrame.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
    "    'Scores': [55, 60, 65, 70, 75],\n",
    "    'Age': [25, 26, 27, 28, 29]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "#                Hours_Studied   Scores       Age\n",
    "# Hours_Studied          1.000000   1.000000  0.400000\n",
    "# Scores                 1.000000   1.000000  0.400000\n",
    "# Age                    0.400000   0.400000  1.000000\n",
    "# ```\n",
    "\n",
    "# - The `df.corr()` method computes the pairwise correlation for all numerical columns in the DataFrame. The result is a correlation matrix, where each value represents the Pearson correlation coefficient between two variables.\n",
    "# - In this example, **Hours_Studied** and **Scores** have a perfect positive correlation (`1.0`), while the correlation with **Age** is moderate (`0.4`).\n",
    "\n",
    "# ### 2. **Using NumPy**:\n",
    "# You can also use the `numpy.corrcoef()` function to calculate the correlation coefficient between two variables.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example data (Hours_Studied and Scores)\n",
    "hours_studied = np.array([1, 2, 3, 4, 5])\n",
    "scores = np.array([55, 60, 65, 70, 75])\n",
    "\n",
    "# Calculate Pearson correlation coefficient\n",
    "correlation = np.corrcoef(hours_studied, scores)\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "# [[1. 1.]\n",
    "#  [1. 1.]]\n",
    "# ```\n",
    "\n",
    "# - The `np.corrcoef()` function returns a 2x2 matrix. The value at position `[0, 1]` (or `[1, 0]`) gives the Pearson correlation coefficient between the two arrays. Here, the correlation coefficient is `1.0`, which indicates a perfect positive linear relationship between **Hours_Studied** and **Scores**.\n",
    "\n",
    "# ### 3. **Visualizing Correlation with a Heatmap**:\n",
    "# If you want to visualize the correlation between multiple variables, a heatmap is a great way to do so. You can use the `seaborn` library to plot a heatmap of the correlation matrix.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
    "    'Scores': [55, 60, 65, 70, 75],\n",
    "    'Age': [25, 26, 27, 28, 29]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# A heatmap will be displayed, showing the correlation coefficients between all pairs of variables in the dataset, with annotations indicating the exact correlation values.\n",
    "\n",
    "# ### 4. **Correlation Between Specific Pairs of Variables**:\n",
    "# If you want to calculate the correlation between specific columns of the dataset, you can do so by passing the columns directly to the `.corr()` method.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "# Calculate correlation between 'Hours_Studied' and 'Scores' only\n",
    "correlation_value = df['Hours_Studied'].corr(df['Scores'])\n",
    "print(correlation_value)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "# 1.0\n",
    "# ```\n",
    "\n",
    "# This will return the correlation coefficient between the **Hours_Studied** and **Scores** columns (which is `1.0` in this case).\n",
    "\n",
    "# ### Summary:\n",
    "# - **`.corr()` method** in **pandas** computes the pairwise correlation between columns in a DataFrame.\n",
    "# - **`numpy.corrcoef()`** calculates the correlation coefficient between two arrays.\n",
    "# - **Seaborn heatmaps** are a great way to visualize the correlation matrix for multiple variables.\n",
    "\n",
    "# These methods provide a powerful way to explore and understand the relationships between variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cb2e6c7-5a3f-4f05-8eaa-36787d5adfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 15 >> What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    "# ### **What is Causation?**\n",
    "\n",
    "# **Causation** refers to a relationship where one variable directly influences or causes a change in another variable. In other words, a causal relationship means that a change in one variable leads to a change in the other. This relationship implies that there is a cause-and-effect link between the two variables.\n",
    "\n",
    "# ### **Correlation vs. Causation:**\n",
    "\n",
    "# While **correlation** indicates that two variables are related in some way (either positively or negatively), **causation** goes a step further and implies that one variable causes the change in another.\n",
    "\n",
    "# #### **Key Differences:**\n",
    "# 1. **Correlation**:\n",
    "#    - Indicates that two variables are associated or related in some way.\n",
    "#    - The relationship could be positive, negative, or zero, but it does not imply that one variable causes the other to change.\n",
    "#    - Correlation can be **spurious** or coincidental. It might just be a statistical relationship without any real cause-and-effect link.\n",
    "\n",
    "# 2. **Causation**:\n",
    "#    - Implies that one variable **directly** causes a change in another.\n",
    "#    - A causal relationship suggests that **if** one variable changes, **then** the other will change as a result, under certain conditions.\n",
    "#    - Causality requires evidence beyond just correlation and is often supported by experiments or controlled studies that can rule out other influencing factors.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Example to Illustrate the Difference:**\n",
    "\n",
    "# #### **Correlation Example:**\n",
    "# There is a strong positive correlation between the number of ice creams sold and the number of people who go swimming at the beach. As ice cream sales increase, the number of swimmers also tends to increase.\n",
    "\n",
    "# - **Correlation**: Ice cream sales and the number of swimmers are correlated.\n",
    "# - **Why it's not causation**: It does not mean that buying more ice cream **causes** people to go swimming. There is likely a **third factor** (e.g., hot weather) influencing both variables. On hot days, more people go swimming, and simultaneously, more ice cream is sold. The relationship between these variables is driven by the weather.\n",
    "\n",
    "# #### **Causation Example:**\n",
    "# Consider an experiment where you test whether increasing the amount of study time causes an improvement in test scores. If, in the experiment, you control for other factors and observe that increasing study time leads to higher test scores, you can claim **causation**.\n",
    "\n",
    "# - **Causation**: More study time **causes** better test scores.\n",
    "# - **Why this is causation**: In this case, there is a direct cause-and-effect relationship between the two variables. If you increase the amount of time spent studying, the improvement in test scores is observed as a result.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Visualizing the Key Differences:**\n",
    "\n",
    "# 1. **Correlation**:\n",
    "#    - Ice cream sales ⟷ Swimming at the beach (related due to a third factor, but no direct cause-and-effect).\n",
    "   \n",
    "# 2. **Causation**:\n",
    "#    - Study time ⟶ Test scores (study time directly influences the test scores).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Why the Difference Matters:**\n",
    "\n",
    "# - **Correlation** might suggest a relationship, but it doesn't explain why or how the variables are related. Correlation can happen due to random chance, external factors, or a hidden variable.\n",
    "# - **Causation** involves understanding the underlying mechanism and can lead to actionable insights. For example, knowing that more study time causes higher test scores allows you to take specific actions to improve performance.\n",
    "\n",
    "# ### **In Practice**:\n",
    "# - **Correlation** is often detected first (using statistical methods like Pearson's correlation) to explore potential relationships between variables.\n",
    "# - **Causation** is more difficult to prove and usually requires **controlled experiments** (e.g., randomized controlled trials, A/B testing) or advanced statistical techniques like **causal inference** models (e.g., Granger causality, instrumental variables).\n",
    "\n",
    "# ### **Summary**:\n",
    "# - **Correlation** means two variables are related, but it doesn't imply one causes the other.\n",
    "# - **Causation** implies a cause-and-effect relationship, where one variable directly influences the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11bfcf6e-a46f-4c94-9ecd-b18d40e10f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 16 >> What is an Optimizer? What are different types of optimizers? Explain each with an example\n",
    "\n",
    "# ### **What is an Optimizer?**\n",
    "\n",
    "# In machine learning and deep learning, an **optimizer** is an algorithm used to minimize or maximize the loss function (or cost function) during the training of a model. Optimizers adjust the model's parameters (weights and biases) in order to reduce the loss and improve the model's performance on unseen data.\n",
    "\n",
    "# An **optimizer** helps in finding the best values for the model’s parameters by iteratively adjusting them based on gradients, which are calculated using techniques like **backpropagation** in neural networks.\n",
    "\n",
    "# ### **Types of Optimizers**\n",
    "\n",
    "# There are several types of optimizers used in training machine learning models, especially in neural networks. Each optimizer has its own mechanism to adjust the model's weights. Here are the most commonly used ones:\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 1. **Gradient Descent (GD)**\n",
    "\n",
    "# **Gradient Descent** is the most basic optimization algorithm. It works by iteratively adjusting the model's parameters in the direction of the negative gradient of the loss function with respect to the parameters. The learning rate determines the size of the step taken in the direction of the gradient.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta)\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(\\theta\\) represents the parameters (weights/biases),\n",
    "# - \\(\\eta\\) is the learning rate,\n",
    "# - \\(\\nabla J(\\theta)\\) is the gradient of the loss function \\(J(\\theta)\\) with respect to \\(\\theta\\).\n",
    "\n",
    "# #### Example:\n",
    "# Imagine you're training a linear regression model. The goal is to minimize the Mean Squared Error (MSE) loss. With each iteration, gradient descent will adjust the weights to reduce the error by following the gradient of the MSE.\n",
    "\n",
    "# - **Advantages**: Simple and intuitive.\n",
    "# - **Disadvantages**: May get stuck in local minima or converge slowly.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "# **Stochastic Gradient Descent (SGD)** is a variant of gradient descent where, instead of calculating the gradient based on the entire dataset, it uses only a **single sample** (or a small batch) to compute the gradient and update the parameters. This makes SGD faster but introduces more variance in the updates.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta; x^{(i)}, y^{(i)})\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(x^{(i)}, y^{(i)}\\) are the individual data point and its corresponding label.\n",
    "\n",
    "# #### Example:\n",
    "# In a neural network, instead of computing the gradient over the entire training set, SGD computes it for just one data point at a time, allowing for quicker updates.\n",
    "\n",
    "# - **Advantages**: Faster per iteration.\n",
    "# - **Disadvantages**: More noisy and less stable updates due to randomness.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 3. **Mini-Batch Gradient Descent**\n",
    "\n",
    "# **Mini-Batch Gradient Descent** is a compromise between **Gradient Descent** and **Stochastic Gradient Descent**. Instead of using the entire dataset or just one data point, it uses a **small subset** (mini-batch) of the training data to calculate the gradient. This method speeds up the process and reduces the noise compared to SGD.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# \\theta = \\theta - \\eta \\cdot \\nabla J(\\theta; X_{\\text{batch}}, Y_{\\text{batch}})\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(X_{\\text{batch}}, Y_{\\text{batch}}\\) are a mini-batch of data points and their labels.\n",
    "\n",
    "# #### Example:\n",
    "# In a deep learning model, you might train using a mini-batch of size 32 or 64 (instead of using the entire dataset or a single data point) to compute the gradient and update weights.\n",
    "\n",
    "# - **Advantages**: Faster and more stable than SGD.\n",
    "# - **Disadvantages**: Still requires tuning the mini-batch size and learning rate.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 4. **Momentum**\n",
    "\n",
    "# **Momentum** is an enhancement to gradient descent that helps the optimizer avoid local minima and speeds up convergence. It accumulates an exponentially decaying moving average of past gradients, which allows the optimizer to keep moving in the same direction for a longer period of time, thus accelerating convergence.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta)\n",
    "# \\]\n",
    "# \\[\n",
    "# \\theta = \\theta - \\eta v_t\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(v_t\\) is the velocity (momentum),\n",
    "# - \\(\\beta\\) is the momentum coefficient (typically close to 0.9),\n",
    "# - \\(\\nabla J(\\theta)\\) is the gradient.\n",
    "\n",
    "# #### Example:\n",
    "# Momentum is often used in deep learning models, like CNNs, where each update is smoothed out based on previous updates, leading to faster convergence.\n",
    "\n",
    "# - **Advantages**: Helps avoid oscillations and local minima.\n",
    "# - **Disadvantages**: Requires tuning the momentum parameter.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 5. **AdaGrad (Adaptive Gradient Algorithm)**\n",
    "\n",
    "# **AdaGrad** adapts the learning rate for each parameter individually, based on the historical gradients. It gives smaller updates for parameters associated with frequently occurring features and larger updates for parameters associated with infrequent features. This is useful when dealing with sparse data (e.g., natural language processing).\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# \\theta = \\theta - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla J(\\theta)\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(G_t\\) is the sum of the squared gradients for each parameter up to time step \\(t\\),\n",
    "# - \\(\\epsilon\\) is a small constant to prevent division by zero.\n",
    "\n",
    "# #### Example:\n",
    "# AdaGrad is often used in models with sparse features, like text classification tasks using bag-of-words, where some words are much more common than others.\n",
    "\n",
    "# - **Advantages**: Adapts learning rates for each parameter.\n",
    "# - **Disadvantages**: The learning rate may become too small after many updates, leading to slow convergence.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 6. **RMSProp (Root Mean Square Propagation)**\n",
    "\n",
    "# **RMSProp** is an optimizer designed to address the problem of AdaGrad's decaying learning rate. It uses a moving average of the squared gradients to scale the learning rate for each parameter.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta)^2\n",
    "# \\]\n",
    "# \\[\n",
    "# \\theta = \\theta - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\cdot \\nabla J(\\theta)\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(v_t\\) is the moving average of squared gradients,\n",
    "# - \\(\\beta\\) is the decay factor.\n",
    "\n",
    "# #### Example:\n",
    "# RMSProp is widely used in training deep neural networks, especially for problems like training recurrent neural networks (RNNs), where AdaGrad struggles with rapidly decaying learning rates.\n",
    "\n",
    "# - **Advantages**: Handles non-stationary objectives and adapts learning rates effectively.\n",
    "# - **Disadvantages**: Requires tuning the decay parameter \\(\\beta\\).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 7. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "# **Adam** combines the benefits of both **Momentum** and **RMSProp**. It maintains a moving average of both the gradients and the squared gradients (first and second moments), allowing it to adapt the learning rate efficiently.\n",
    "\n",
    "# #### Formula:\n",
    "# \\[\n",
    "# m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla J(\\theta)\n",
    "# \\]\n",
    "# \\[\n",
    "# v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla J(\\theta)^2\n",
    "# \\]\n",
    "# \\[\n",
    "# \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "# \\]\n",
    "# \\[\n",
    "# \\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "# \\]\n",
    "# Where:\n",
    "# - \\(m_t\\) and \\(v_t\\) are the first and second moment estimates,\n",
    "# - \\(\\beta_1\\) and \\(\\beta_2\\) are decay rates for the moving averages,\n",
    "# - \\(\\epsilon\\) is a small constant to prevent division by zero.\n",
    "\n",
    "# #### Example:\n",
    "# Adam is the most commonly used optimizer in deep learning for tasks like image classification with CNNs, language modeling with RNNs, and many others.\n",
    "\n",
    "# - **Advantages**: Combines the benefits of momentum and adaptive learning rates. Very popular due to its good performance in a wide variety of problems.\n",
    "# - **Disadvantages**: Requires careful tuning of hyperparameters.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Optimizers:\n",
    "\n",
    "# | Optimizer        | Description                                      | Use Case                   |\n",
    "# |------------------|--------------------------------------------------|----------------------------|\n",
    "# | **Gradient Descent**   | Basic, uses entire dataset for updates.          | Simple problems with small datasets. |\n",
    "# | **Stochastic Gradient Descent (SGD)** | Uses single sample for updates.          | Faster, especially for large datasets. |\n",
    "# | **Mini-Batch Gradient Descent** | Uses a small batch of data for updates.    | Common for deep learning tasks. |\n",
    "# | **Momentum**         | Adds velocity to updates for faster convergence.  | Helps avoid local minima. |\n",
    "# | **AdaGrad**          | Adapts learning rates for each parameter.         | Works well with sparse data. |\n",
    "# | **RMSProp**          | Improves AdaGrad by using moving averages.        | Popular for RNNs and NLP tasks. |\n",
    "# | **Adam**             | Combines Momentum and RMSProp for adaptive learning. | Works well for a wide range of deep learning tasks. |\n",
    "\n",
    "# ---\n",
    "\n",
    "# Each optimizer has its own strengths and weaknesses, and choosing the right one depends on the nature of the problem, dataset, and model being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b80e40-8afe-499c-ba8a-72af21e90fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 17 >> What is sklearn.linear_model\n",
    "# The `sklearn.linear_model` module in **scikit-learn** (a popular machine learning library in Python) contains a variety of algorithms for **linear modeling**, which are widely used for predictive modeling tasks. Linear models make predictions based on a linear relationship between input features (predictors) and the target variable (response). These models are foundational for regression and classification tasks.\n",
    "\n",
    "# ### **Key Models in `sklearn.linear_model`**\n",
    "\n",
    "# Here are some of the most commonly used models available in the `sklearn.linear_model` module:\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 1. **Linear Regression (`LinearRegression`)**\n",
    "# - **Purpose**: Used for **regression** tasks where the goal is to predict a continuous target variable based on the linear relationship between the independent variables (features) and the target.\n",
    "  \n",
    "# - **Formula**: \n",
    "#   \\[\n",
    "#   y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(y\\) is the predicted target.\n",
    "#   - \\(x_1, x_2, ..., x_n\\) are the input features.\n",
    "#   - \\(\\beta_0, \\beta_1, ..., \\beta_n\\) are the model coefficients (weights).\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]  # Input features\n",
    "y = [1, 2, 3, 4, 5]            # Target variable\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict using the fitted model\n",
    "predictions = model.predict([[6]])\n",
    "print(predictions)  # Expected output: [6.]\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 2. **Ridge Regression (`Ridge`)**\n",
    "# - **Purpose**: Ridge regression is a form of **regularized linear regression** that adds a penalty to the magnitude of coefficients to prevent overfitting. It’s particularly useful when there’s multicollinearity (high correlation between independent variables).\n",
    "\n",
    "# - **Formula** (with L2 regularization):\n",
    "#   \\[\n",
    "#   \\min_{\\beta} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2 \\right)\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(\\alpha\\) is the regularization parameter (higher \\(\\alpha\\) reduces the magnitude of coefficients).\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize the model with regularization parameter alpha\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model to the data\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = ridge_model.predict([[6]])\n",
    "print(predictions)  # Expected output: [6.]\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 3. **Lasso Regression (`Lasso`)**\n",
    "# - **Purpose**: Lasso regression is another type of **regularized linear regression**, but it uses **L1 regularization**, which adds a penalty based on the absolute value of coefficients. This leads to **sparse solutions**, where some coefficients may become exactly zero, effectively performing feature selection.\n",
    "\n",
    "# - **Formula** (with L1 regularization):\n",
    "#   \\[\n",
    "#   \\min_{\\beta} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\sum_{j=1}^{p} |\\beta_j| \\right)\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(\\alpha\\) is the regularization parameter (higher \\(\\alpha\\) results in more coefficients being zeroed out).\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize the model with regularization parameter alpha\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model to the data\n",
    "lasso_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lasso_model.predict([[6]])\n",
    "print(predictions)  # Expected output: [6.]\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 4. **ElasticNet (`ElasticNet`)**\n",
    "# - **Purpose**: ElasticNet is a hybrid regularized linear model that combines both **L1 (Lasso)** and **L2 (Ridge)** penalties. It’s useful when there are multiple features with **high correlations**.\n",
    "\n",
    "# - **Formula**:\n",
    "#   \\[\n",
    "#   \\min_{\\beta} \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\alpha \\left( \\rho \\sum_{j=1}^{p} |\\beta_j| + (1-\\rho) \\sum_{j=1}^{p} \\beta_j^2 \\right) \\right)\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(\\rho\\) is the mixing parameter between L1 and L2 regularization, and \\(\\alpha\\) controls the strength of regularization.\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize the model with regularization parameters alpha and rho\n",
    "elasticnet_model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "# Fit the model to the data\n",
    "elasticnet_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = elasticnet_model.predict([[6]])\n",
    "print(predictions)  # Expected output: [6.]\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 5. **Logistic Regression (`LogisticRegression`)**\n",
    "# - **Purpose**: Used for **classification** tasks, Logistic Regression predicts the probability of a binary outcome (0 or 1). It uses the **logistic function (sigmoid function)** to model the relationship between input features and the probability of the target variable.\n",
    "\n",
    "# - **Formula**:\n",
    "#   \\[\n",
    "#   p(y = 1 | X) = \\frac{1}{1 + \\exp(-(w \\cdot X + b))}\n",
    "#   \\]\n",
    "#   Where:\n",
    "#   - \\(w \\cdot X + b\\) is the linear combination of the features.\n",
    "#   - The output is a probability between 0 and 1.\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Example dataset (X = features, y = binary target)\n",
    "X = [[1], [2], [3], [4]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "# Initialize the model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "logistic_model.fit(X, y)\n",
    "\n",
    "# Make predictions (probabilities)\n",
    "predictions = logistic_model.predict([[5]])\n",
    "print(predictions)  # Expected output: [1] (prediction for class 1)\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### 6. **RANSAC Regressor (`RANSACRegressor`)**\n",
    "# - **Purpose**: The **RANSAC** (Random Sample Consensus) algorithm is a robust linear regression model that is used when the data contains **outliers**. It repeatedly fits a linear model on random subsets of the data, while identifying and excluding outliers.\n",
    "\n",
    "# - **Example**:\n",
    "\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "# Example dataset with some outliers\n",
    "X = [[1], [2], [3], [4], [5], [10]]\n",
    "y = [1, 2, 3, 4, 5, 100]  # Outlier at X = 10\n",
    "\n",
    "# Initialize RANSAC model\n",
    "ransac_model = RANSACRegressor()\n",
    "\n",
    "# Fit the model to the data\n",
    "ransac_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = ransac_model.predict([[6]])\n",
    "print(predictions)  # Expected output: prediction without outlier influence\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Summary of Key Models in `sklearn.linear_model`:\n",
    "\n",
    "# | Model                    | Task Type    | Description                                         |\n",
    "# |--------------------------|--------------|-----------------------------------------------------|\n",
    "# | **LinearRegression**      | Regression   | Simple linear regression without regularization.    |\n",
    "# | **Ridge**                 | Regression   | Linear regression with L2 regularization.           |\n",
    "# | **Lasso**                 | Regression   | Linear regression with L1 regularization.           |\n",
    "# | **ElasticNet**            | Regression   | Linear regression combining L1 and L2 regularization. |\n",
    "# | **LogisticRegression**    | Classification| Logistic regression for binary classification tasks. |\n",
    "# | **RANSACRegressor**       | Regression   | Robust regression model that handles outliers.      |\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### Conclusion:\n",
    "# `sklearn.linear_model` provides various models to perform both **regression** and **classification** tasks using linear techniques, each with its own strengths (e.g., regularization, robustness against outliers). These models are highly useful for building predictive models in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fdb1bbf-4cb4-4af1-8ca5-4eb0319aae03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa61c40-f802-4d4c-b2de-1c35e508ef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 18 >> What does model.fit() do? What arguments must be given\n",
    "\n",
    "# ### **What does `model.fit()` do?**\n",
    "\n",
    "# In machine learning, the `fit()` method is used to **train a model** on the provided dataset. It takes in the data and adjusts the model's parameters (weights and biases) to learn the underlying patterns in the data. This method is called once to train the model using the training data, enabling it to make predictions or classify new, unseen data.\n",
    "\n",
    "# ### **Arguments Required for `model.fit()`**\n",
    "\n",
    "# The arguments for the `fit()` method depend on the specific type of model being used (e.g., regression or classification). However, for most machine learning models, the basic required arguments are:\n",
    "\n",
    "# 1. **X (Features/Input Data)**\n",
    "#    - **Description**: This is the **input data** or **feature matrix**, where each row represents an observation or data point, and each column represents a feature (variable).\n",
    "#    - **Type**: Typically, a 2D array-like structure (e.g., a NumPy array, Pandas DataFrame, or a matrix).\n",
    "#      - For example, in a supervised learning problem, if you're predicting house prices based on features like the number of rooms, square footage, etc., `X` would be the matrix with rows as houses and columns as the respective features.\n",
    "\n",
    "\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5]]  # 4 samples, 2 features each\n",
    "\n",
    "\n",
    "# 2. **y (Target/Labels)**\n",
    "#    - **Description**: This is the **target variable** or **labels** that you want to predict. It could be a continuous value (in regression) or a class label (in classification).\n",
    "#    - **Type**: Typically, a 1D array-like structure (e.g., a list, NumPy array, or Pandas Series).\n",
    "#      - For example, in a house price prediction problem, `y` would be the house prices corresponding to each row in `X`.\n",
    "\n",
    " \n",
    "y = [100000, 150000, 200000, 250000]  # Corresponding target values (e.g., house prices)\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Basic Example Using `fit()`**\n",
    "\n",
    "# Let's look at an example using **Linear Regression**:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Input features (X) - let's assume we have 4 data points with 2 features each.\n",
    "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "\n",
    "# Target values (y)\n",
    "y = [1, 2, 3, 4]\n",
    "\n",
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using the fit() method\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "# - In this example:\n",
    "#   - `X` contains the features (input data), which could represent the number of rooms and square footage of a house, for instance.\n",
    "#   - `y` contains the target values (house prices).\n",
    "#   - The `model.fit(X, y)` statement tells the model to learn the best-fitting parameters (coefficients) for the given features and target values.\n",
    "\n",
    "# ### **Optional Arguments**\n",
    "\n",
    "# While `X` and `y` are required, many machine learning models accept **additional optional parameters** in the `fit()` method. Some of these parameters might include:\n",
    "\n",
    "# - **sample_weight** (optional): Specifies a weight for each training sample, which can be useful when some data points are more important than others.\n",
    "# - **epochs** (optional, in some models): Specifies the number of iterations for training (for models like neural networks).\n",
    "# - **batch_size** (optional, in some models): Defines the size of data batches used during training (relevant for deep learning models).\n",
    "\n",
    "# Example with `sample_weight`:\n",
    "\n",
    "model.fit(X, y, sample_weight=[1, 2, 1, 3])\n",
    "\n",
    "\n",
    "# This would apply different weights to each training sample when fitting the model.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Summary of `fit()` Method Arguments**\n",
    "\n",
    "# - **Required arguments**:\n",
    "#   1. **X**: Input features (a 2D array-like structure).\n",
    "#   2. **y**: Target variable (a 1D array-like structure).\n",
    "\n",
    "# - **Optional arguments**:\n",
    "#   - `sample_weight`: Weights for individual data points (if needed).\n",
    "#   - Other hyperparameters depending on the model (e.g., `epochs`, `batch_size` in deep learning models).\n",
    "\n",
    "# In short, `model.fit()` trains the model by using the input data (`X`) and the target values (`y`) to learn the relationship and adjust the model's parameters, allowing it to make predictions later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c8aa4c-2544-49ad-a999-dcacb591c823",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 28\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # question 19 >> What does model.predict() do? What arguments must be given\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ### **What does `model.predict()` do?**\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Let's use **Linear Regression** as an example:\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Training data (X_train and y_train)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m X_train \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m]]  \u001b[38;5;66;03m# Features for training\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# # question 19 >> What does model.predict() do? What arguments must be given\n",
    "\n",
    "# ### **What does `model.predict()` do?**\n",
    "\n",
    "# The `model.predict()` method in machine learning is used to **make predictions** based on a trained model. After the model has been trained using the `fit()` method, you can use `predict()` to generate predictions (or classifications) for new, unseen data (i.e., test data or data you want to make predictions for).\n",
    "\n",
    "# - **For regression models**: `predict()` will output continuous values (predictions).\n",
    "# - **For classification models**: `predict()` will output class labels or probabilities, depending on the model's configuration.\n",
    "\n",
    "# In essence, `model.predict()` uses the **parameters** (such as coefficients or weights) that were learned during training to make predictions on new data.\n",
    "\n",
    "# ### **Arguments for `model.predict()`**\n",
    "\n",
    "# The main argument that the `predict()` method takes is:\n",
    "\n",
    "# 1. **X (Features/Input Data)**\n",
    "#    - **Description**: This is the new **input data** for which you want to make predictions. The data should be in the same format (shape, type, and structure) as the data used during training (e.g., same number of features, same dimensionality).\n",
    "#    - **Type**: Typically, a 2D array-like structure (e.g., NumPy array, Pandas DataFrame, or matrix).\n",
    "#      - **Shape**: If you're predicting for multiple samples, `X` should have shape `(n_samples, n_features)`, where `n_samples` is the number of new data points, and `n_features` is the number of features (columns) per data point.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Example Using `predict()`**\n",
    "\n",
    "# Let's use **Linear Regression** as an example:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data (X_train and y_train)\n",
    "X_train = [[1], [2], [3], [4], [5]]  # Features for training\n",
    "y_train = [1, 2, 3, 4, 5]            # Target values for training\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# New data to make predictions on\n",
    "X_new = [[6], [7]]  # New data points for which we want predictions\n",
    "\n",
    "# Use the trained model to predict\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Output the predictions\n",
    "print(predictions)  # Output will be the predicted target values for X_new\n",
    "\n",
    "\n",
    "# - **Input Data (`X_new`)**: This is the new data for which you want predictions.\n",
    "# - **Output (`predictions`)**: This is the predicted target values for the given input data.\n",
    "\n",
    "# ### **Example Output**:\n",
    "# For the input `[[6], [7]]`, since the model learned that the relationship is linear (`y = x`), the output would be:\n",
    "# ```\n",
    "# [6. 7.]\n",
    "# ```\n",
    "\n",
    "# ### **For Classification Models**:\n",
    "\n",
    "# If you're working with a **classification model**, such as Logistic Regression, `predict()` will return predicted class labels for the input data.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training data for classification (X_train and y_train)\n",
    "X_train = [[1], [2], [3], [4], [5]]\n",
    "y_train = [0, 0, 1, 1, 1]  # Binary class labels (0 or 1)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# New data to predict class labels\n",
    "X_new = [[2.5], [4.5]]\n",
    "\n",
    "# Use the trained model to predict class labels\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "# Output the predictions\n",
    "print(predictions)  # Output will be the predicted class labels for X_new\n",
    "\n",
    "\n",
    "# - **Input Data (`X_new`)**: New data points for which you want to predict class labels.\n",
    "# - **Output (`predictions`)**: Predicted class labels (e.g., 0 or 1 in binary classification).\n",
    "\n",
    "# ### **Example Output**:\n",
    "# For the input `[[2.5], [4.5]]`, the output could be:\n",
    "# ```\n",
    "# [0 1]\n",
    "# ```\n",
    "# This indicates that the model predicted class label 0 for the first input and class label 1 for the second input.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Summary of `model.predict()` Arguments**\n",
    "\n",
    "# - **Required Argument**:\n",
    "#   - `X`: The **input data** (features) for which predictions are to be made. The format should be the same as the training data (typically a 2D array-like structure).\n",
    "\n",
    "# - **Output**:\n",
    "#   - For **regression** models: The output will be continuous values (predictions).\n",
    "#   - For **classification** models: The output will be class labels (or probabilities, depending on the model configuration).\n",
    "\n",
    "# ### **Important Notes**:\n",
    "\n",
    "# - Ensure that the shape of the input data (`X`) passed to `predict()` matches the shape of the data used during training. For example, if you trained the model on 2 features, the new data passed to `predict()` should also have 2 features.\n",
    "# - `predict()` works for making predictions on unseen data, so you can evaluate the model's performance on test data or make predictions for future data.\n",
    "\n",
    "# In summary, `model.predict()` is used to generate predictions from a trained model based on new data, and it requires only the feature data (`X`) for those predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88a64803-cf4d-4f5f-b95d-6000c459200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 20 >> What are continuous and categorical variables\n",
    "\n",
    "# ### **Continuous and Categorical Variables**\n",
    "\n",
    "# In data science and machine learning, variables (or features) can be classified into two main types: **continuous** and **categorical** variables. Understanding these types of variables is crucial because it influences how we process and analyze data, as well as the choice of algorithms used in machine learning models.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Continuous Variables**\n",
    "# - **Definition**: Continuous variables are numeric variables that can take an **infinite number of values** within a given range. These variables are measured and can represent quantities that are **countable or measurable**. Their values can be whole numbers or decimals, and they often arise from measurements.\n",
    "  \n",
    "# - **Examples**:\n",
    "#   - **Height** (e.g., 5.6 feet, 5.72 feet, 6.0 feet, etc.)\n",
    "#   - **Weight** (e.g., 50.5 kg, 75.2 kg)\n",
    "#   - **Temperature** (e.g., 22.5°C, 30.7°C)\n",
    "#   - **Time** (e.g., 12.35 seconds, 2.1 hours)\n",
    "\n",
    "# - **Characteristics**:\n",
    "#   - They can take any value within a range (e.g., between 0 and 100, or even negative values like temperature).\n",
    "#   - Typically represented as **real numbers** (including integers and decimals).\n",
    "#   - **Measurement unit** matters, e.g., meters, seconds, kilograms, etc.\n",
    "\n",
    "# - **How to Handle**: Continuous variables can be used directly in most machine learning algorithms, but they may need to be scaled or normalized for certain algorithms like neural networks, support vector machines, or k-nearest neighbors.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Categorical Variables**\n",
    "# - **Definition**: Categorical variables represent **categories or labels**. They take a limited, fixed number of possible values, each representing a specific group or class. These variables often describe qualities or characteristics that don’t have inherent numerical meaning.\n",
    "\n",
    "# - **Types of Categorical Variables**:\n",
    "#   1. **Nominal**: Categories with no intrinsic order or ranking between them.\n",
    "#      - **Examples**: \n",
    "#        - **Gender** (e.g., male, female)\n",
    "#        - **Color** (e.g., red, blue, green)\n",
    "#        - **Country** (e.g., USA, Canada, India)\n",
    "#   2. **Ordinal**: Categories with a defined order or ranking, but the intervals between them are not meaningful.\n",
    "#      - **Examples**:\n",
    "#        - **Education Level** (e.g., high school, bachelor's, master's, Ph.D.)\n",
    "#        - **Customer Satisfaction** (e.g., very dissatisfied, dissatisfied, neutral, satisfied, very satisfied)\n",
    "#        - **Rating Scale** (e.g., 1 to 5 stars)\n",
    "  \n",
    "# - **Characteristics**:\n",
    "#   - **Nominal variables**: Have no natural order (e.g., color or country).\n",
    "#   - **Ordinal variables**: Have a natural order, but the difference between the categories is not defined (e.g., ranking levels).\n",
    "#   - Categorical variables are typically represented as **strings** (e.g., \"red\", \"blue\", \"high school\") or **numbers** (used as labels, but without meaningful numerical differences).\n",
    "\n",
    "# - **How to Handle**: Categorical variables often need to be converted into a numerical format before using them in most machine learning algorithms. This is commonly done through techniques like:\n",
    "#   - **One-Hot Encoding**: Creates binary columns for each category (e.g., gender might be split into \"Male\" and \"Female\").\n",
    "#   - **Label Encoding**: Assigns each category a unique integer (e.g., \"red\" → 1, \"blue\" → 2).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Summary of Differences**\n",
    "\n",
    "# | Feature                | Continuous Variables                              | Categorical Variables                               |\n",
    "# |------------------------|---------------------------------------------------|----------------------------------------------------|\n",
    "# | **Nature**             | Numeric, can take any value within a range       | Categorical, with a finite set of categories       |\n",
    "# | **Examples**           | Height, Weight, Temperature, Age                 | Gender, Country, Education Level, Satisfaction    |\n",
    "# | **Representation**     | Real numbers (decimals or integers)              | Strings or integers representing categories       |\n",
    "# | **Mathematical Operations** | Can perform arithmetic operations (e.g., addition, subtraction) | Cannot perform arithmetic operations (e.g., addition) |\n",
    "# | **Subtypes**           | None                                             | Nominal, Ordinal                                  |\n",
    "# | **Machine Learning Handling** | Usually requires scaling or normalization     | Often requires encoding (e.g., one-hot encoding)   |\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Real-World Example:**\n",
    "# Imagine you're building a model to predict house prices.\n",
    "\n",
    "# - **Continuous variables**: \n",
    "#   - **Square footage** (e.g., 1200 sqft, 1500 sqft)\n",
    "#   - **Number of bedrooms** (e.g., 3, 4)\n",
    "#   - **Price** (e.g., $200,000, $350,000)\n",
    "\n",
    "# - **Categorical variables**:\n",
    "#   - **Neighborhood** (e.g., Downtown, Suburb, Rural)\n",
    "#   - **Type of house** (e.g., Detached, Semi-Detached, Apartment)\n",
    "#   - **Condition** (e.g., New, Renovated, Needs Repair)\n",
    "\n",
    "# Understanding the nature of your variables allows you to prepare the data appropriately, choose the correct algorithms, and interpret your results meaningfully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db25677-caa2-4392-9706-80e281dc6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # question 21 >> What is feature scaling? How does it help in Machine Learning\n",
    "\n",
    "# **Feature scaling** is the process of standardizing or normalizing the range of independent variables (features) in a dataset. The goal of feature scaling is to transform the features so that they are on a similar scale, which helps improve the performance and training of machine learning algorithms. \n",
    "\n",
    "# ### Types of Feature Scaling:\n",
    "# 1. **Normalization (Min-Max Scaling)**:\n",
    "#    - This technique scales the features to a fixed range, usually [0, 1].\n",
    "#    - Formula: \n",
    "#      \\[\n",
    "#      X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "#      \\]\n",
    "#    - It is particularly useful when the data does not follow a Gaussian distribution and when the algorithm involves distance metrics (e.g., KNN, SVM).\n",
    "\n",
    "# 2. **Standardization (Z-Score Normalization)**:\n",
    "#    - This method scales the features so that they have a mean of 0 and a standard deviation of 1.\n",
    "#    - Formula:\n",
    "#      \\[\n",
    "#      X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "#      \\]\n",
    "#      where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the feature.\n",
    "#    - Standardization is commonly used when the features have different units or when the data follows a Gaussian distribution.\n",
    "\n",
    "# ### Why Feature Scaling is Important in Machine Learning:\n",
    "# 1. **Improved Algorithm Performance**:\n",
    "#    - Some machine learning algorithms, especially those that rely on distances (like KNN, SVM, and gradient descent), are sensitive to the scale of the features. Features with larger numerical ranges can dominate the learning process, leading to biased models.\n",
    "   \n",
    "# 2. **Faster Convergence in Gradient-Based Algorithms**:\n",
    "#    - Gradient descent-based algorithms (such as logistic regression, neural networks, and linear regression) converge faster when the features are scaled. When features have varying scales, the gradient updates can be inconsistent, making the model training slower or causing it to get stuck in local minima.\n",
    "\n",
    "# 3. **Equal Importance of Features**:\n",
    "#    - Feature scaling ensures that all features contribute equally to the model. Without scaling, features with larger ranges might disproportionately influence the model, while features with smaller ranges might be neglected.\n",
    "\n",
    "# ### When to Use Feature Scaling:\n",
    "# - **Distance-Based Algorithms**: Algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM) require scaling since they use distance measures (e.g., Euclidean distance).\n",
    "# - **Gradient-Based Algorithms**: Algorithms such as logistic regression, neural networks, and linear regression benefit from scaling to ensure faster convergence.\n",
    "# - **Clustering Algorithms**: Algorithms like K-Means use distance measures and thus should also benefit from feature scaling.\n",
    "\n",
    "# ### Conclusion:\n",
    "# Feature scaling is crucial to ensure that machine learning models perform optimally, especially when features have different units or scales. It helps prevent bias in distance-based models, speeds up convergence in gradient-based algorithms, and ensures the models treat all features equally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685ceef-9da7-45f9-92df-b51de258127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 22 >> How do we perform scaling in Python?\n",
    "\n",
    "# In Python, scaling refers to adjusting the range of data, typically for machine learning or data processing tasks. There are different ways to scale data, and commonly used methods include:\n",
    "\n",
    "# 1. **Min-Max Scaling**: This technique scales the data to a specific range, often between 0 and 1. The formula used is:\n",
    "#    \\[\n",
    "#    X_{\\text{scaled}} = \\frac{X - \\min(X)}{\\max(X) - \\min(X)}\n",
    "#    \\]\n",
    "\n",
    "# 2. **Standardization (Z-score Scaling)**: This method scales the data by subtracting the mean and dividing by the standard deviation, transforming the data to have a mean of 0 and a standard deviation of 1. The formula is:\n",
    "#    \\[\n",
    "#    X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "#    \\]\n",
    "#    where \\( \\mu \\) is the mean and \\( \\sigma \\) is the standard deviation of the data.\n",
    "\n",
    "# ### Using Scikit-learn for Scaling:\n",
    "# The `scikit-learn` library provides easy-to-use functions for scaling data.\n",
    "\n",
    "# #### 1. Min-Max Scaling (using `MinMaxScaler`):\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "\n",
    "#### 2. Standardization (using `StandardScaler`):\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "\n",
    "# #### 3. Robust Scaling (using `RobustScaler`):\n",
    "# Robust scaling is less sensitive to outliers.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Example data\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n",
    "\n",
    "\n",
    "# ### Scaling with Pandas:\n",
    "# If you're working with a DataFrame and want to scale data manually or using pandas, you can use `pandas` operations for scaling.\n",
    "\n",
    "# #### Example of Min-Max Scaling using pandas:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame([[-1, 2], [-0.5, 6], [0, 10], [1, 18]], columns=['Feature1', 'Feature2'])\n",
    "\n",
    "# Min-Max scaling\n",
    "data_scaled = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "print(data_scaled)\n",
    "\n",
    "\n",
    "#### Example of Standardization using pandas:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame([[-1, 2], [-0.5, 6], [0, 10], [1, 18]], columns=['Feature1', 'Feature2'])\n",
    "\n",
    "# Standardization\n",
    "data_scaled = (data - data.mean()) / data.std()\n",
    "\n",
    "print(data_scaled)\n",
    "\n",
    "\n",
    "# ### Choosing the Right Scaling Method:\n",
    "# - **Min-Max Scaling** is useful when the data has a known range and you need to preserve the distribution.\n",
    "# - **Standardization** is often more appropriate when the data follows a Gaussian distribution, or you plan to use algorithms that are sensitive to the scale of the features, like SVM, KNN, or neural networks.\n",
    "# - **Robust Scaling** is better when the data contains many outliers.\n",
    "\n",
    "# Use the appropriate method based on the characteristics of your data and the machine learning model you are working with!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48c1e193-20c1-43f3-9794-74920b253381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 23 >> What is sklearn.preprocessing   (repeated question no 08 )\n",
    "\n",
    "#                Same Answer >> Question No 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc419f2-d076-4c62-a7fe-78c3736e9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 24 >> How do we split data for model fitting (training and testing) in Python\n",
    "\n",
    "# In Python, particularly when using **scikit-learn**, splitting data into training and testing sets is a common practice to evaluate machine learning models effectively. The `train_test_split` function from `scikit-learn` is the most widely used method to do this.\n",
    "\n",
    "# ### Steps to split data for model fitting:\n",
    "\n",
    "# 1. **Import the necessary libraries**.\n",
    "# 2. **Prepare the data** (features and target variable).\n",
    "# 3. **Use `train_test_split` to split the data** into training and testing sets.\n",
    "# 4. **Train a model** using the training data.\n",
    "# 5. **Evaluate the model** using the testing data.\n",
    "\n",
    "# ### Example of how to split data:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load an example dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data  # Features (input variables)\n",
    "y = data.target  # Target variable (output)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# test_size=0.2 means 20% of the data will be used for testing\n",
    "# random_state is used to ensure reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f'Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# ### Parameters for `train_test_split`:\n",
    "\n",
    "# - **X**: Features (input data).\n",
    "# - **y**: Target (output labels).\n",
    "# - **test_size**: Proportion of the data to be used for testing (e.g., 0.2 means 20% for testing and 80% for training).\n",
    "# - **train_size**: Proportion of the data to be used for training. If not specified, it’s inferred from the `test_size`.\n",
    "# - **random_state**: Integer seed for reproducibility. If set, the function will generate the same split each time you run the code.\n",
    "# - **shuffle**: Boolean indicating whether to shuffle the data before splitting. By default, it is `True`.\n",
    "# - **stratify**: This parameter is important when working with imbalanced datasets. If set to the target variable `y`, it ensures the split maintains the same proportion of classes in both the training and testing sets.\n",
    "\n",
    "# ### Example with `stratify` for imbalanced data:\n",
    "\n",
    "# When the target classes are imbalanced, it's often important to ensure that the training and test sets have the same class distribution. The `stratify` parameter helps in this case.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# This ensures that the class distribution in `y_train` and `y_test` mirrors that of the original `y`.\n",
    "\n",
    "# ### Output:\n",
    "\n",
    "# - `X_train`: The training features.\n",
    "# - `X_test`: The testing features.\n",
    "# - `y_train`: The training target.\n",
    "# - `y_test`: The testing target.\n",
    "\n",
    "# ### Notes:\n",
    "# - You can adjust `test_size` based on how much data you want to allocate for testing. Common splits are 80-20 or 70-30 (training-testing).\n",
    "# - The `random_state` ensures that the split can be reproduced. You can set it to any integer or leave it out for a random split each time.\n",
    "\n",
    "# This basic approach works for most use cases when you're preparing data for training machine learning models!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25033b30-4484-477a-86b8-09285bb762fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 25 >> Explain data encoding?\n",
    "\n",
    "# Data encoding refers to the process of converting categorical (non-numeric) data into a numerical format so that it can be used in machine learning algorithms, which typically require numeric input. This transformation is essential when dealing with categorical variables like colors, cities, product categories, etc., which don't directly provide meaningful numeric values for modeling.\n",
    "\n",
    "# There are several common techniques for encoding categorical data:\n",
    "\n",
    "# ### 1. **Label Encoding**\n",
    "# Label encoding involves converting each category into a unique integer. Each label is assigned a distinct integer value based on alphabetical order or any other scheme.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example data\n",
    "categories = ['cat', 'dog', 'cat', 'rabbit']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the categories into numeric labels\n",
    "encoded_labels = label_encoder.fit_transform(categories)\n",
    "\n",
    "print(encoded_labels)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "# [0 1 0 2]\n",
    "# ```\n",
    "\n",
    "# Here, 'cat' is encoded as 0, 'dog' as 1, and 'rabbit' as 2. This encoding works well when there is an inherent ordinal relationship between the categories, such as low, medium, high, etc. But for non-ordinal data, this method may introduce unintended ordinal relationships.\n",
    "\n",
    "# ### 2. **One-Hot Encoding**\n",
    "# One-hot encoding is a more common technique for encoding categorical data where each category is represented as a binary vector (array of 0s and 1s). A new binary feature is created for each possible category value, where each category will be represented by a 1 in its respective column, and 0s elsewhere.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Example data (reshaped for one-hot encoding)\n",
    "categories = np.array(['cat', 'dog', 'cat', 'rabbit']).reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_one_hot = one_hot_encoder.fit_transform(categories)\n",
    "\n",
    "print(encoded_one_hot)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "# [[1. 0. 0.]\n",
    "#  [0. 1. 0.]\n",
    "#  [1. 0. 0.]\n",
    "#  [0. 0. 1.]]\n",
    "# ```\n",
    "\n",
    "# Here:\n",
    "# - The first column corresponds to 'cat'.\n",
    "# - The second column corresponds to 'dog'.\n",
    "# - The third column corresponds to 'rabbit'.\n",
    "\n",
    "# So, the 'cat' category is encoded as `[1, 0, 0]`, 'dog' as `[0, 1, 0]`, and 'rabbit' as `[0, 0, 1]`.\n",
    "\n",
    "# **Advantages**:\n",
    "# - One-hot encoding does not assume any order between categories.\n",
    "# - It avoids the problem of introducing an unintended ordinal relationship.\n",
    "\n",
    "# **Disadvantages**:\n",
    "# - One-hot encoding can significantly increase the dimensionality of the data, especially when there are many categories. This is called the \"curse of dimensionality.\"\n",
    "\n",
    "# ### 3. **Binary Encoding**\n",
    "# Binary encoding is a compromise between label encoding and one-hot encoding. It converts each category into a binary number and then separates the binary digits into individual columns. This is useful when the number of categories is large, as it reduces the number of features compared to one-hot encoding.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "# Example data\n",
    "categories = ['cat', 'dog', 'cat', 'rabbit', 'fish']\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "encoder = ce.BinaryEncoder(cols=['Category'])\n",
    "\n",
    "# Fit and transform\n",
    "df = pd.DataFrame({'Category': categories})\n",
    "encoded_df = encoder.fit_transform(df)\n",
    "\n",
    "print(encoded_df)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "#    Category_0  Category_1\n",
    "# 0           1           0\n",
    "# 1           0           1\n",
    "# 2           1           0\n",
    "# 3           1           1\n",
    "# 4           0           0\n",
    "# ```\n",
    "\n",
    "# ### 4. **Target Encoding (Mean Encoding)**\n",
    "# Target encoding involves encoding categorical variables based on the mean of the target variable. This technique can be especially useful in supervised learning when there is a relationship between the categorical variable and the target.\n",
    "\n",
    "# For example, for each category, you would replace it with the mean value of the target variable for that category.\n",
    "\n",
    "# #### Example:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data (with target column)\n",
    "data = pd.DataFrame({\n",
    "    'Category': ['cat', 'dog', 'cat', 'rabbit', 'dog'],\n",
    "    'Target': [1, 0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# Calculate the mean of the target variable for each category\n",
    "category_means = data.groupby('Category')['Target'].mean()\n",
    "\n",
    "# Map the means to the categories\n",
    "data['Category_encoded'] = data['Category'].map(category_means)\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "#   Category  Target  Category_encoded\n",
    "# 0      cat       1               1.0\n",
    "# 1      dog       0               0.0\n",
    "# 2      cat       1               1.0\n",
    "# 3   rabbit       1               1.0\n",
    "# 4      dog       0               0.0\n",
    "# ```\n",
    "\n",
    "# Here, the category 'cat' has a mean target of 1.0, 'dog' has a mean target of 0.0, and 'rabbit' has a mean target of 1.0.\n",
    "\n",
    "# ### 5. **Frequency Encoding**\n",
    "# Frequency encoding replaces the categories with the frequency (count) of occurrences of each category in the dataset. This can be useful when there is a high cardinality of categories.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = pd.DataFrame({\n",
    "    'Category': ['cat', 'dog', 'cat', 'rabbit', 'dog', 'dog', 'rabbit']\n",
    "})\n",
    "\n",
    "# Frequency encoding\n",
    "category_freq = data['Category'].value_counts()\n",
    "data['Category_encoded'] = data['Category'].map(category_freq)\n",
    "\n",
    "print(data)\n",
    "\n",
    "\n",
    "# #### Output:\n",
    "# ```\n",
    "#   Category  Category_encoded\n",
    "# 0      cat                 2\n",
    "# 1      dog                 3\n",
    "# 2      cat                 2\n",
    "# 3   rabbit                 2\n",
    "# 4      dog                 3\n",
    "# 5      dog                 3\n",
    "# 6   rabbit                 2\n",
    "# ```\n",
    "\n",
    "# ### Choosing the Right Encoding Method:\n",
    "# - **Label Encoding**: Suitable for ordinal categories where there is a meaningful order (e.g., 'low', 'medium', 'high').\n",
    "# - **One-Hot Encoding**: Ideal for nominal categories where no ordinal relationship exists.\n",
    "# - **Binary Encoding**: Useful for high-cardinality categorical data, as it reduces dimensionality compared to one-hot encoding.\n",
    "# - **Target Encoding**: Effective when there is a relationship between the category and the target variable.\n",
    "# - **Frequency Encoding**: Helpful when you want to capture the distribution of categories in the dataset.\n",
    "\n",
    "# ### Summary:\n",
    "# Data encoding is an important preprocessing step that ensures machine learning models can interpret categorical data. The choice of encoding technique depends on the type of data and the machine learning algorithm you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7332e-cb35-4668-a36f-1d86aeb82d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
